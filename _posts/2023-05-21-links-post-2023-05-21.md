---
comments: true
date: 2023-05-21 17:00:00 +0500
author: muness
title: '2023-05-21 Links Post'
---

# 2023-05-21 Links Post

Some links to things I found interesting this week:

- [BI by Another Name](https://benn.substack.com/p/bi-by-another-name) by Benn Stancil.
  > If we’re going to build universal semantic layers, they have to be flexible enough—like SQL itself—to work with tools that present data in a range of different ways. Otherwise, we’re not building semantic layers; we’re building BI tools by another name.

  My takeway: Current metric layers are failing. This is because they should present metrics in a way that can be consumed by any type of data tool. In other words, they should support SQL. This (IMO) is why dbt Labs acquired MetricFlow as it supports [querying metrics using sql](https://docs.transform.co/docs/metricflow/guides/introduction#accessing-metrics) and passes through other queries [directly to a SQL engine](https://docs.transform.co/docs/api/sql/#:~:text=run%20any%20supported%20SQL%20that%20your%20data%20warehouse%20supports%20alongside%20these%20requests.).
- [Michael Easter recommends pull ups](https://www.twopct.com/p/dont-die-do-pullups)
  > Pullups mobilize your shoulders and strengthen your back muscles, pulling you into a less slumped position and preventing issues down the road.

  I've never been able to do a pull up, now I want to get to being able to do just _one_.
- [Google AI Infrastructure Supremacy: Systems Matter More Than Microarchitecture](https://www.semianalysis.com/p/google-ai-infrastructure-supremacy#%C2%A7the-largest-at-scale-ai-model-architecture-dlrm) mentioned that recommender models used by the big tech companies are far bigger than LLM models:
  > DLRMs are the backbone of companies like Baidu, Meta, ByteDance, Netflix, and Google. It is the engine of over a trillion dollars of annual revenue in advertising, search ranking, social media feed ordering, et. These models consist of billions of weights, training on more than a trillion examples and [handling inference at over 300,000 queries per second](https://www.semianalysis.com/p/the-inference-cost-of-search-disruption). **The size of these models (10TB+) and far exceeds that of even the largest transformer models, such as GPT4, which is on the order of 1TB+ (model architecture differences).**
- Gaping Void reminds us that [Innovation isn't Orthodox](https://www.gapingvoid.com/blog/2023/05/05/innovation-isnt-orthodox/) and that cultures wishing to foster innova them need to welcome challenging ideas:
  > Great cultures are not fragile. They are not afraid of change, and they’re not afraid to see their ideas challenged, even the good ones. They welcome new ideas and new people into the mix.
- The 2018 essay [Neural Network Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526) is the best explanation I've yet to come across of embeddings:
  > An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers... Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.

  Which makes me wonder: Did we essentially brute force a solution to the [Frames Problem](https://en.wikipedia.org/wiki/Frame_problem) using embeddings?
- Macro Hive's [Asset Allocation Update](https://macrohive.com/asset-allocation/new-look-asset-allocation-update/) (paid) states that we've entered a high inflation regime:
  > we are in a high inflation regime where wage and price behaviours have become impacted by already high inflation... in high inflation regimes, price inflation becomes high because wage inflation has become high and vice versa. Prices across most categories become correlated and shocks, such as energy price increases, spill over into the whole economy.
- [Poe's Law](https://www.dictionary.com/e/slang/poes-law) was one I've seen plenty of, but didn't know has been named:
  > fundamentalist or dogmatic views can become so extreme, despite their acceptance, that even parodies of this views are unmistakable for the real thing, to the point that extremists may accidentally embrace a parody as truth.
- A [Q&A](https://www.twopct.com/p/q-and-a-problems-with-optimizing) by Michael Easter has a sidebar on the futility of optimizing complex systems:
  > **It’s impossible to optimize complex, constantly changing biological systems that are all different. And even if we could reach some sort of optimal state in some facet of our life, we’d never know it.** Trying to optimize a system that can’t be optimized can lead us into misery. We’ll always be searching for some end destination that doesn’t exist

  > **Instead of asking, “How do I optimize X,” ask yourself, “What problem am I trying to solve?”** That question often leads to a more targeted and effective solution. You may even realize there’s no problem in the first place
- Bryan Caplan discusses the [marginal cost](https://betonit.substack.com/p/the-cost-is-zero-or-astronomical) of adding a student:
  > The naive view of the world is that the cost of every individual equals the average cost. The sophisticated view is that the cost of the *typical* individual equals zero. The even *more* sophisticated view, though, is that the cost of the typical individual is either roughly zero - or almost astronomical.

  Don't fret though, because:

  > sometimes the marginal cost of a good remains zero over an enormous range.

  My takeaway is: this is one reason all those spreadsheets on how a system will scale are bunk: There's a massive step function somewhere that you're going to miss.
- Seth Godin describes [one reason](https://seths.blog/2023/05/the-thing-about-decay/) Tech Debt cleanup efforts fail:
  >  If we simply put effort on top of a shaky foundation, it’ll all be wasted.

- [Byrne Hobart —  Over- And Under-Correction in Tech](https://cactus.substack.com/p/byrne-hobart-over-and-under-correction#details) had a passing comment on how we "anthropomorphize humans too much". I've been pondering how we assume others think like we do or that we can put ourselves in someone else's shoes. This framing on the phenomena was really great.

Until next time!
